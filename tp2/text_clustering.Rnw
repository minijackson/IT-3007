\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[hidelinks]{hyperref}
\usepackage{amsmath}
\usepackage{fullpage}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{biblatex}
\usepackage{float}

\title{Text Analysis and Data Clustering}
\author{RÃ©mi NICOLE}

\addbibresource{text_clustering-tutorials.bib}
\addbibresource{text_clustering.bib}

\begin{document}

\maketitle

\part{Introduction}

<<libraries, message=FALSE, warnings=FALSE>>=
library(tm)
data(acq)
library(SnowballC)
library(knitr)
library(ggplot2)
library(gplots)
library(RColorBrewer)
library(wordcloud)
library(cluster)
library(fpc)
library(topicmodels)
@

\part{Cleaning and Stemming}

\paragraph{}
In order to clean the text, we have to remove the punctuation, the numbers,
the stop-words of the English language and convert the text into all-lowercase
text.

\paragraph{}
We will also "stem" the text, which means that every word will be stripped
down to the radical of the word. For example, the word "saying" will be
replaced by "say".

<<cleanStem>>=
acq.clean <- tm_map(acq, removePunctuation)
acq.clean <- tm_map(acq.clean, removeNumbers)
acq.clean <- tm_map(acq.clean, tolower)
acq.clean <- tm_map(acq.clean, removeWords, c(stopwords("english"), "said", "will"))
acq.clean <- tm_map(acq.clean, stripWhitespace)
acq.clean <- tm_map(acq.clean, PlainTextDocument)
acq.clean <- tm_map(acq.clean, stemDocument)
# Print a sample of the cleaned text
str(acq.clean[1][[1]]$content, width=80, strict.width="cut")
@

\part{Document term matrices}

\paragraph{}
Then, we transform that data into a `DocumentTermMatrix` which is a matrix of
occurrences of all the words stored in the data. We then sum the columns of
the matrix to obtain the number of occurrences of the words.

<<dtm>>=
acq.dtm <- DocumentTermMatrix(acq.clean)
acq.freqs <- colSums(as.matrix(acq.dtm))
acq.freqs <- data.frame(Word=names(acq.freqs),
                        Frequency=acq.freqs)
@

<<topFrequencies, echo=FALSE>>=
kable(head(acq.freqs[order(acq.freqs$Frequency, decreasing=TRUE),c(1,2)], n=10), caption="Top frequent words in the acq dataset", booktabs=TRUE, row.names=FALSE)
@

<<histogram, fig.cap="Histogram of the most used words", fig.pos="H">>=
acq.freqs.small <- acq.freqs[tail(order(acq.freqs$Frequency), n=25), c(1,2)]
ggplot(acq.freqs.small, aes(Word, Frequency)) +
	geom_bar(stat="identity") +
	theme(axis.text.x=element_text(angle=45, hjust=1))
@

<<wordcloud, fig.cap="Wordcloud of the acq dataset", fig.pos="H">>=
wordcloud(acq.freqs$Word,
          acq.freqs$Frequency,
          max.words=100,
          scale=c(5, .1),
          colors=brewer.pal(9, "Blues")[4:9])
@

\part{Hierarchical clustering}

\paragraph{}
From this part on, we will remove the sparse words. As our matrix is very
sparse, we will only decrease the sparsity to 80\% to have enough data to have
meaningful clustering.

<<distanceAndFitting>>=
acq.dtms <- removeSparseTerms(acq.dtm, .8)
acq.distances <- dist(t(acq.dtms), method="euclidian")
acq.hfit <- hclust(acq.distances, method="ward.D2")
@

<<heatmap, fig.cap="Heatmap of the distance between the words", fig.pos="H">>=
heatmap.2(as.matrix(acq.distances), trace="none")
@

<<dendogram, fig.cap="Clustered dendogram of the relations between the words", fig.pos="H">>=
plot(acq.hfit, hang=-1)
rect.hclust(acq.hfit, k=7, border="gray")
@

\part{K-means clustering}

<<clusterPlot>>=
acq.kfit <- kmeans(acq.distances, 5, 500)
clusplot(as.matrix(acq.distances), acq.kfit$cluster, color=T, shade=T, labels=2)
@

\part{Topic models}

<<topicFinding>>=
acq.lda <- LDA(acq.dtm, k=7)
acq.terms <- terms(acq.lda, 4)
acq.terms <- apply(acq.terms, MARGIN=2, paste, collapse=", ")

acq.topics <- data.frame(document=1:50, topics(acq.lda))
@

<<topicTermsTable, echo=FALSE>>=
kable(acq.terms, caption="Topic terms", booktabs=TRUE, valign="H")
@

<<documentTopics>>=
acq.topicsWKeywods <-
	data.frame(Document=1:50,
	           Topic.Keywords=acq.terms[acq.topics$topics.acq.lda.])
@

<<documentTopicsTable, echo=FALSE>>=
kable(acq.topicsWKeywods, caption="Topic terms by document", booktabs=TRUE, longtable=TRUE, valign="H")
@

<<automaticBibliography, echo=FALSE>>=
write_bib(sub("^.*/", "", grep("^/", searchpaths(), value=TRUE)),
          file="text_clustering.bib")
@

\nocite{*}
\printbibliography

\end{document}

% Thank you: http://fr.slideshare.net/rdatamining/text-mining-with-r-an-analysis-of-twitter-data
